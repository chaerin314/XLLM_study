{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1684,
     "status": "ok",
     "timestamp": 1743567533645,
     "user": {
      "displayName": "­박채린 / 학생 / 수리과학부",
      "userId": "13191912736177007141"
     },
     "user_tz": -540
    },
    "id": "DgNLyayqU7KS",
    "outputId": "57d577ff-28cb-4869-fdf5-e908a5077059"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\XLLM_study\\book1\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.nn.Transformer()\n",
    "model.encoder.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1743561017989,
     "user": {
      "displayName": "­박채린 / 학생 / 수리과학부",
      "userId": "13191912736177007141"
     },
     "user_tz": -540
    },
    "id": "DzUP7wzpVJ8T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m      model(*args, **kwargs)\n",
      "\u001b[31mType:\u001b[39m           Transformer\n",
      "\u001b[31mString form:\u001b[39m   \n",
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\user\\desktop\\xllm_study\\book1\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\n",
      "\u001b[31mSource:\u001b[39m        \n",
      "\u001b[38;5;28;01mclass\u001b[39;00m Transformer(Module):\n",
      "    \u001b[33mr\"\"\"A transformer model.\u001b[39m\n",
      "\n",
      "\u001b[33m    .. note::\u001b[39m\n",
      "\u001b[33m        See `this tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\u001b[39m\n",
      "\u001b[33m        for an in depth discussion of the performant building blocks PyTorch offers for building your own\u001b[39m\n",
      "\u001b[33m        transformer layers.\u001b[39m\n",
      "\n",
      "\u001b[33m    User is able to modify the attributes as needed. The architecture\u001b[39m\n",
      "\u001b[33m    is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer,\u001b[39m\n",
      "\u001b[33m    Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\u001b[39m\n",
      "\u001b[33m    Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\u001b[39m\n",
      "\u001b[33m    Processing Systems, pages 6000-6010.\u001b[39m\n",
      "\n",
      "\u001b[33m    Args:\u001b[39m\n",
      "\u001b[33m        d_model: the number of expected features in the encoder/decoder inputs (default=512).\u001b[39m\n",
      "\u001b[33m        nhead: the number of heads in the multiheadattention models (default=8).\u001b[39m\n",
      "\u001b[33m        num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).\u001b[39m\n",
      "\u001b[33m        num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).\u001b[39m\n",
      "\u001b[33m        dim_feedforward: the dimension of the feedforward network model (default=2048).\u001b[39m\n",
      "\u001b[33m        dropout: the dropout value (default=0.1).\u001b[39m\n",
      "\u001b[33m        activation: the activation function of encoder/decoder intermediate layer, can be a string\u001b[39m\n",
      "\u001b[33m            (\"relu\" or \"gelu\") or a unary callable. Default: relu\u001b[39m\n",
      "\u001b[33m        custom_encoder: custom encoder (default=None).\u001b[39m\n",
      "\u001b[33m        custom_decoder: custom decoder (default=None).\u001b[39m\n",
      "\u001b[33m        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\u001b[39m\n",
      "\u001b[33m        batch_first: If ``True``, then the input and output tensors are provided\u001b[39m\n",
      "\u001b[33m            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\u001b[39m\n",
      "\u001b[33m        norm_first: if ``True``, encoder and decoder layers will perform LayerNorms before\u001b[39m\n",
      "\u001b[33m            other attention and feedforward operations, otherwise after. Default: ``False`` (after).\u001b[39m\n",
      "\u001b[33m        bias: If set to ``False``, ``Linear`` and ``LayerNorm`` layers will not learn an additive\u001b[39m\n",
      "\u001b[33m            bias. Default: ``True``.\u001b[39m\n",
      "\n",
      "\u001b[33m    Examples::\u001b[39m\n",
      "\u001b[33m        >>> transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\u001b[39m\n",
      "\u001b[33m        >>> src = torch.rand((10, 32, 512))\u001b[39m\n",
      "\u001b[33m        >>> tgt = torch.rand((20, 32, 512))\u001b[39m\n",
      "\u001b[33m        >>> out = transformer_model(src, tgt)\u001b[39m\n",
      "\n",
      "\u001b[33m    Note: A full example to apply nn.Transformer module for the word language model is available in\u001b[39m\n",
      "\u001b[33m    https://github.com/pytorch/examples/tree/master/word_language_model\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(\n",
      "        self,\n",
      "        d_model: int = \u001b[32m512\u001b[39m,\n",
      "        nhead: int = \u001b[32m8\u001b[39m,\n",
      "        num_encoder_layers: int = \u001b[32m6\u001b[39m,\n",
      "        num_decoder_layers: int = \u001b[32m6\u001b[39m,\n",
      "        dim_feedforward: int = \u001b[32m2048\u001b[39m,\n",
      "        dropout: float = \u001b[32m0.1\u001b[39m,\n",
      "        activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
      "        custom_encoder: Optional[Any] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        custom_decoder: Optional[Any] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        layer_norm_eps: float = \u001b[32m1e-5\u001b[39m,\n",
      "        batch_first: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        norm_first: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        bias: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "        device=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        dtype=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        factory_kwargs = {\u001b[33m\"device\"\u001b[39m: device, \u001b[33m\"dtype\"\u001b[39m: dtype}\n",
      "        super().__init__()\n",
      "        torch._C._log_api_usage_once(f\"torch.nn.modules.{self.__class__.__name__}\")\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m custom_encoder \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            self.encoder = custom_encoder\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            encoder_layer = TransformerEncoderLayer(\n",
      "                d_model,\n",
      "                nhead,\n",
      "                dim_feedforward,\n",
      "                dropout,\n",
      "                activation,\n",
      "                layer_norm_eps,\n",
      "                batch_first,\n",
      "                norm_first,\n",
      "                bias,\n",
      "                **factory_kwargs,\n",
      "            )\n",
      "            encoder_norm = LayerNorm(\n",
      "                d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs\n",
      "            )\n",
      "            self.encoder = TransformerEncoder(\n",
      "                encoder_layer, num_encoder_layers, encoder_norm\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m custom_decoder \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            self.decoder = custom_decoder\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            decoder_layer = TransformerDecoderLayer(\n",
      "                d_model,\n",
      "                nhead,\n",
      "                dim_feedforward,\n",
      "                dropout,\n",
      "                activation,\n",
      "                layer_norm_eps,\n",
      "                batch_first,\n",
      "                norm_first,\n",
      "                bias,\n",
      "                **factory_kwargs,\n",
      "            )\n",
      "            decoder_norm = LayerNorm(\n",
      "                d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs\n",
      "            )\n",
      "            self.decoder = TransformerDecoder(\n",
      "                decoder_layer, num_decoder_layers, decoder_norm\n",
      "            )\n",
      "\n",
      "        self._reset_parameters()\n",
      "\n",
      "        self.d_model = d_model\n",
      "        self.nhead = nhead\n",
      "\n",
      "        self.batch_first = batch_first\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m forward(\n",
      "        self,\n",
      "        src: Tensor,\n",
      "        tgt: Tensor,\n",
      "        src_mask: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        tgt_mask: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        memory_mask: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        src_key_padding_mask: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        tgt_key_padding_mask: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        memory_key_padding_mask: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        src_is_causal: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        tgt_is_causal: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        memory_is_causal: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    ) -> Tensor:\n",
      "        \u001b[33mr\"\"\"Take in and process masked source/target sequences.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. note::\u001b[39m\n",
      "\n",
      "\u001b[33m            If a boolean tensor is provided for any of the [src/tgt/memory]_mask arguments, positions with a ``True`` value are\u001b[39m\n",
      "\u001b[33m            not allowed to participate in the attention,\u001b[39m\n",
      "\u001b[33m            which is the opposite of the definition for :attr:`attn_mask`\u001b[39m\n",
      "\u001b[33m            in :func:`torch.nn.functional.scaled_dot_product_attention`.\u001b[39m\n",
      "\n",
      "\u001b[33m        Args:\u001b[39m\n",
      "\u001b[33m            src: the sequence to the encoder (required).\u001b[39m\n",
      "\u001b[33m            tgt: the sequence to the decoder (required).\u001b[39m\n",
      "\u001b[33m            src_mask: the additive mask for the src sequence (optional).\u001b[39m\n",
      "\u001b[33m            tgt_mask: the additive mask for the tgt sequence (optional).\u001b[39m\n",
      "\u001b[33m            memory_mask: the additive mask for the encoder output (optional).\u001b[39m\n",
      "\u001b[33m            src_key_padding_mask: the Tensor mask for src keys per batch (optional).\u001b[39m\n",
      "\u001b[33m            tgt_key_padding_mask: the Tensor mask for tgt keys per batch (optional).\u001b[39m\n",
      "\u001b[33m            memory_key_padding_mask: the Tensor mask for memory keys per batch (optional).\u001b[39m\n",
      "\u001b[33m            src_is_causal: If specified, applies a causal mask as ``src_mask``.\u001b[39m\n",
      "\u001b[33m                Default: ``None``; try to detect a causal mask.\u001b[39m\n",
      "\u001b[33m                Warning:\u001b[39m\n",
      "\u001b[33m                ``src_is_causal`` provides a hint that ``src_mask`` is\u001b[39m\n",
      "\u001b[33m                the causal mask. Providing incorrect hints can result in\u001b[39m\n",
      "\u001b[33m                incorrect execution, including forward and backward\u001b[39m\n",
      "\u001b[33m                compatibility.\u001b[39m\n",
      "\u001b[33m            tgt_is_causal: If specified, applies a causal mask as ``tgt_mask``.\u001b[39m\n",
      "\u001b[33m                Default: ``None``; try to detect a causal mask.\u001b[39m\n",
      "\u001b[33m                Warning:\u001b[39m\n",
      "\u001b[33m                ``tgt_is_causal`` provides a hint that ``tgt_mask`` is\u001b[39m\n",
      "\u001b[33m                the causal mask. Providing incorrect hints can result in\u001b[39m\n",
      "\u001b[33m                incorrect execution, including forward and backward\u001b[39m\n",
      "\u001b[33m                compatibility.\u001b[39m\n",
      "\u001b[33m            memory_is_causal: If specified, applies a causal mask as\u001b[39m\n",
      "\u001b[33m                ``memory_mask``.\u001b[39m\n",
      "\u001b[33m                Default: ``False``.\u001b[39m\n",
      "\u001b[33m                Warning:\u001b[39m\n",
      "\u001b[33m                ``memory_is_causal`` provides a hint that\u001b[39m\n",
      "\u001b[33m                ``memory_mask`` is the causal mask. Providing incorrect\u001b[39m\n",
      "\u001b[33m                hints can result in incorrect execution, including\u001b[39m\n",
      "\u001b[33m                forward and backward compatibility.\u001b[39m\n",
      "\n",
      "\u001b[33m        Shape:\u001b[39m\n",
      "\u001b[33m            - src: :math:`(S, E)` for unbatched input, :math:`(S, N, E)` if `batch_first=False` or\u001b[39m\n",
      "\u001b[33m              `(N, S, E)` if `batch_first=True`.\u001b[39m\n",
      "\u001b[33m            - tgt: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\u001b[39m\n",
      "\u001b[33m              `(N, T, E)` if `batch_first=True`.\u001b[39m\n",
      "\u001b[33m            - src_mask: :math:`(S, S)` or :math:`(N\\cdot\\text{num\\_heads}, S, S)`.\u001b[39m\n",
      "\u001b[33m            - tgt_mask: :math:`(T, T)` or :math:`(N\\cdot\\text{num\\_heads}, T, T)`.\u001b[39m\n",
      "\u001b[33m            - memory_mask: :math:`(T, S)`.\u001b[39m\n",
      "\u001b[33m            - src_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\u001b[39m\n",
      "\u001b[33m            - tgt_key_padding_mask: :math:`(T)` for unbatched input otherwise :math:`(N, T)`.\u001b[39m\n",
      "\u001b[33m            - memory_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\u001b[39m\n",
      "\n",
      "\u001b[33m            Note: [src/tgt/memory]_mask ensures that position :math:`i` is allowed to attend the unmasked\u001b[39m\n",
      "\u001b[33m            positions. If a BoolTensor is provided, positions with ``True``\u001b[39m\n",
      "\u001b[33m            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\u001b[39m\n",
      "\u001b[33m            is provided, it will be added to the attention weight.\u001b[39m\n",
      "\u001b[33m            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\u001b[39m\n",
      "\u001b[33m            the attention. If a BoolTensor is provided, the positions with the\u001b[39m\n",
      "\u001b[33m            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\u001b[39m\n",
      "\n",
      "\u001b[33m            - output: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\u001b[39m\n",
      "\u001b[33m              `(N, T, E)` if `batch_first=True`.\u001b[39m\n",
      "\n",
      "\u001b[33m            Note: Due to the multi-head attention architecture in the transformer model,\u001b[39m\n",
      "\u001b[33m            the output sequence length of a transformer is same as the input sequence\u001b[39m\n",
      "\u001b[33m            (i.e. target) length of the decoder.\u001b[39m\n",
      "\n",
      "\u001b[33m            where :math:`S` is the source sequence length, :math:`T` is the target sequence length, :math:`N` is the\u001b[39m\n",
      "\u001b[33m            batch size, :math:`E` is the feature number\u001b[39m\n",
      "\n",
      "\u001b[33m        Examples:\u001b[39m\n",
      "\u001b[33m            >>> # xdoctest: +SKIP\u001b[39m\n",
      "\u001b[33m            >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        is_batched = src.dim() == \u001b[32m3\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.batch_first \u001b[38;5;28;01mand\u001b[39;00m src.size(\u001b[32m1\u001b[39m) != tgt.size(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mand\u001b[39;00m is_batched:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\u001b[33m\"the batch number of src and tgt must be equal\"\u001b[39m)\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.batch_first \u001b[38;5;28;01mand\u001b[39;00m src.size(\u001b[32m0\u001b[39m) != tgt.size(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mand\u001b[39;00m is_batched:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\u001b[33m\"the batch number of src and tgt must be equal\"\u001b[39m)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m src.size(-\u001b[32m1\u001b[39m) != self.d_model \u001b[38;5;28;01mor\u001b[39;00m tgt.size(-\u001b[32m1\u001b[39m) != self.d_model:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
      "                \u001b[33m\"the feature number of src and tgt must be equal to d_model\"\u001b[39m\n",
      "            )\n",
      "\n",
      "        memory = self.encoder(\n",
      "            src,\n",
      "            mask=src_mask,\n",
      "            src_key_padding_mask=src_key_padding_mask,\n",
      "            is_causal=src_is_causal,\n",
      "        )\n",
      "        output = self.decoder(\n",
      "            tgt,\n",
      "            memory,\n",
      "            tgt_mask=tgt_mask,\n",
      "            memory_mask=memory_mask,\n",
      "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
      "            memory_key_padding_mask=memory_key_padding_mask,\n",
      "            tgt_is_causal=tgt_is_causal,\n",
      "            memory_is_causal=memory_is_causal,\n",
      "        )\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\n",
      "    @staticmethod\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m generate_square_subsequent_mask(\n",
      "        sz: int,\n",
      "        device: Optional[torch.device] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        dtype: Optional[torch.dtype] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ) -> Tensor:\n",
      "        \u001b[33mr\"\"\"Generate a square causal mask for the sequence.\u001b[39m\n",
      "\n",
      "\u001b[33m        The masked positions are filled with float('-inf'). Unmasked positions are filled with float(0.0).\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m _generate_square_subsequent_mask(sz, dtype=dtype, device=device)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _reset_parameters(self):\n",
      "        \u001b[33mr\"\"\"Initiate parameters in the transformer model.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;28;01min\u001b[39;00m self.parameters():\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m p.dim() > \u001b[32m1\u001b[39m:\n",
      "                xavier_uniform_(p)\n",
      "\u001b[31mInit docstring:\u001b[39m Initialize internal Module state, shared by both nn.Module and ScriptModule."
     ]
    }
   ],
   "source": [
    "model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dZfsk6IRVqiG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.4495762084031236)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "small_dots = [\n",
    "    np.dot(np.random.randn(10),\n",
    "           np.random.randn(10))\n",
    "    for i in range(100)]\n",
    "np.mean(np.absolute(small_dots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(81.7489227075375)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_dots = [np.dot(np.random.randn(10000),\n",
    "                     np.random.randn(10000))\n",
    "              for i in range(100)]\n",
    "np.mean(np.absolute(large_dots))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMdepGM/t6RZTfX+rGJ0nN1",
   "mount_file_id": "1KoUtH02CzHdnKtxchIadyEVngQBmSRQq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "book1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
