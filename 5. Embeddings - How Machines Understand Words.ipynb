{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.zeros(20000)\n",
    "o[1152] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7756\\3400914490.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  o = torch.tensor(o).to(torch.int64)\n"
     ]
    }
   ],
   "source": [
    "# int32에서 int64로 변환(아니면 아래 코드 에러 남)\n",
    "o = torch.tensor(o).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = torch.nn.Embedding(num_embeddings = 20000, embedding_dim = 300)\n",
    "e = E(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optim\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "from torchtext import *\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors='glove.6B.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up fields\n",
    "TEXT = data.Field(lower=True, include_lengths=True, \\\n",
    "batch_first=False, tokenize='spacy')\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# Make splits for data\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# Build the vocabulary\n",
    "TEXT.build_vocab(train, vectors='glove.6B.100d') \\\n",
    "# use 'glove.42B.300d' for greater accuracy or \\\n",
    "'glove.6B.100d' for greater speed\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "# Make iterator for splits\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test), \\\n",
    "batch_sizes=(128,1024), device=dev, sort_within_batch=True, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1223321366.py, line 6)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mand copy the weights from the pretrained glove embeddings\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class RNN_classifier(nn.Module):\n",
    "    def __init__(self, embedding_size = 100, hidden_size = 512, num_layers = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up an embedding layer with the right dimensions, \\\n",
    "        and copy the weights from the pretrained glove embeddings\n",
    "        vocab = TEXT.vocab\n",
    "        self.embed = nn.Embedding(len(vocab), embedding_size).cuda()\n",
    "        self.embed.weight.data.copy_(vocab.vectors)\n",
    "\n",
    "        # Set up a standard PyTorch RNN sections with the right \\\n",
    "        dimensions and a variable number of layers\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers)\n",
    "\n",
    "        # Add a two layer classification head with the right dimensions. \\\n",
    "        The final layer must output a single number\n",
    "        self.classificationLayer1 = nn.Linear(hidden_size,10)\n",
    "        self.classificationLayer2 = nn.Linear(10,1)\n",
    "\n",
    "\n",
    "    def forward(self, input, lengths=None):\n",
    "\n",
    "        embed_input = self.embed(input)\n",
    "        packed_emb = nn.utils.rnn.pack_padded_sequence(embed_input,\\\n",
    "        lengths, batch_first=False)\n",
    "\n",
    "        output, hidden = self.rnn(packed_emb)\n",
    "        hidden = hidden[-1]\n",
    "        x = hidden.squeeze(0)\n",
    "        x = self.classificationLayer1(x)\n",
    "        x = self.classificationLayer2(x)\n",
    "\n",
    "        logits = x.view(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_classifier(hidden_size=256, num_layers=1)\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_iter:\n",
    "    (x,x_len) = batch.text\n",
    "    pred = model(x,x_len)\n",
    "    print(pred.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.binary_cross_entropy_with_logits\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, test_data):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(test_data):\n",
    "            text, text_lengths = batch_data.text\n",
    "            logits = model(text, text_lengths)\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            total += batch_data.label.size(0)\n",
    "            correct += (predicted_labels == batch_data.label.long()).sum()\n",
    "        return correct.float()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_iter):\n",
    "        (x,x_lengths)=batch.text\n",
    "        pred = model(x,x_lengths)\n",
    "\n",
    "        actual=batch.label.float()\n",
    "        loss = loss_func(pred,actual)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    if (epoch==5):\n",
    "        for g in opt.param_groups:\n",
    "            g['lr'] = 3e-3\n",
    "\n",
    "    print(\"Accuracy: \" + str(get_metrics(model, test_iter).cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    # based on:\n",
    "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
    "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "\n",
    "    tensor = torch.LongTensor(indexed).to(dev)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"I like that Far From Home is trying something new and that its\n",
    "humor  feels more real than the ironic cracks in most superhero movies.\n",
    "I just wish its good pieces all came together more satisfyingly.\"\"\"\n",
    "\n",
    "print('Probability positive:')\n",
    "predict_sentiment(model, review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
